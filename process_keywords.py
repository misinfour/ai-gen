import json
import argparse
import os
import time
from datetime import datetime, timezone, timedelta
from kv_manager import kv_read, kv_write
from api_manager import MultiPlatformApiManager, ApiExhaustedRetriesError
from config_manager import ConfigManager

# å®šä¹‰åŒ—äº¬æ—¶é—´æ—¶åŒº
beijing_tz = timezone(timedelta(hours=8))

def extract_all_valid_titles(title_text):
    """ä»AIç”Ÿæˆçš„å¤šä¸ªæ ‡é¢˜ä¸­æå–æ‰€æœ‰æœ‰æ•ˆçš„æ ‡é¢˜"""
    if not title_text or not isinstance(title_text, str):
        return [], "æ ‡é¢˜ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯"
    
    valid_titles = []
    # æŒ‰è¡Œåˆ†å‰²ï¼Œå¤„ç†å¤šä¸ªæ ‡é¢˜çš„æƒ…å†µ
    lines = title_text.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # å»é™¤åºå·ï¼ˆå¦‚ "1. "ã€"2. " ç­‰ï¼‰
        import re
        line = re.sub(r'^\d+\.\s*', '', line)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸¤ä¸ª----åˆ†éš”ç¬¦
        parts = line.split('----')
        if len(parts) == 3:
            # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†æ˜¯å¦ä¸ºç©º
            title, custom_tail, game_name = parts
            if title.strip() and custom_tail.strip() and game_name.strip():
                valid_titles.append(line)
    
    if valid_titles:
        return valid_titles, f"æ‰¾åˆ° {len(valid_titles)} ä¸ªæœ‰æ•ˆæ ‡é¢˜"
    else:
        return [], "æ²¡æœ‰æ‰¾åˆ°æ ¼å¼æ­£ç¡®çš„æ ‡é¢˜"

def validate_title_format(title_text):
    """éªŒè¯æ ‡é¢˜æ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸç»“æ„ï¼šæ ‡é¢˜----è‡ªå®šä¹‰å°¾è¯----æ¸¸æˆå"""
    valid_titles, message = extract_all_valid_titles(title_text)
    if valid_titles:
        return True, valid_titles
    else:
        return False, message

def generate_title(prompt, keyword, config_manager, api_manager):
    """ä½¿ç”¨AIç”Ÿæˆæ–‡ç« æ ‡é¢˜"""
    import time
    
    max_retries = 3
    retry_delay = 5  # ç§’
    
    for attempt in range(max_retries):
        try:
            # å‡†å¤‡æç¤ºè¯ï¼Œæ›¿æ¢{GameWord}å˜é‡
            actual_prompt = prompt.replace('{GameWord}', keyword)
            
            # ä½¿ç”¨å¤šå¹³å°APIç®¡ç†å™¨ç”Ÿæˆå†…å®¹
            print(f"æ­£åœ¨ä¸ºå…³é”®è¯ '{keyword}' ç”Ÿæˆæ ‡é¢˜... (å°è¯• {attempt + 1}/{max_retries})")
            content = api_manager.make_request(actual_prompt)
            
            # æ¸…ç†è¿”å›çš„å†…å®¹ï¼Œåˆ é™¤<think>æ ‡ç­¾
            import re
            cleaned_text = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
            cleaned_text = cleaned_text.strip()
            
            # éªŒè¯æ ‡é¢˜æ ¼å¼å¹¶æå–æ‰€æœ‰æœ‰æ•ˆæ ‡é¢˜
            is_valid, valid_titles = validate_title_format(cleaned_text)
            if is_valid:
                print(f"âœ… æ ‡é¢˜æ ¼å¼éªŒè¯é€šè¿‡: æ‰¾åˆ° {len(valid_titles)} ä¸ªæœ‰æ•ˆæ ‡é¢˜")
                for i, title in enumerate(valid_titles[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªæ ‡é¢˜
                    print(f"   {i}. {title}")
                if len(valid_titles) > 3:
                    print(f"   ... è¿˜æœ‰ {len(valid_titles) - 3} ä¸ªæ ‡é¢˜")
                return valid_titles
            else:
                print(f"âš ï¸ æ ‡é¢˜æ ¼å¼éªŒè¯å¤±è´¥: {valid_titles}")
                print(f"   ç”Ÿæˆçš„åŸå§‹å†…å®¹: {cleaned_text[:200]}...")
                if attempt < max_retries - 1:
                    print(f"   å°†é‡è¯•ç”Ÿæˆ...")
                    continue
                else:
                    return f"æ ¼å¼éªŒè¯å¤±è´¥: {valid_titles}"
            
        except ApiExhaustedRetriesError:
            # APIé‡è¯•è€—å°½å¼‚å¸¸ç›´æ¥ä¼ é€’ï¼Œä¸è¿›è¡Œå†…éƒ¨é‡è¯•
            print(f"ğŸ”¥ ç”Ÿæˆæ ‡é¢˜æ—¶APIé‡è¯•è€—å°½ï¼Œç«‹å³ç»ˆæ­¢: {keyword}")
            raise
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ ç”Ÿæˆæ ‡é¢˜å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç†”æ–­ç›¸å…³çš„å¼‚å¸¸
            if any(keyword in error_msg for keyword in ['ç†”æ–­æœºåˆ¶', 'è¿ç»­å¤±è´¥', 'ApiExhaustedRetriesError', 'ğŸ”¥']):
                print(f"ğŸ”¥ æ£€æµ‹åˆ°ç†”æ–­ç›¸å…³å¼‚å¸¸ï¼Œç«‹å³ç»ˆæ­¢")
                raise e
            
            if attempt < max_retries - 1:
                print(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
            else:
                print(f"ğŸ’¥ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œè·³è¿‡å…³é”®è¯ '{keyword}'")
                return f"ç”Ÿæˆå¤±è´¥: {error_msg}"

def find_latest_kv_data(account_id, namespace_id, api_token, max_days_back=30):
    """æŸ¥æ‰¾KVå­˜å‚¨ä¸­æœ€æ–°å­˜åœ¨çš„æ•°æ®
    
    Args:
        account_id: Cloudflareè´¦æˆ·ID
        namespace_id: KVå‘½åç©ºé—´ID
        api_token: APIä»¤ç‰Œ
        max_days_back: æœ€å¤šå‘å‰æŸ¥æ‰¾å¤šå°‘å¤©
    
    Returns:
        tuple: (kv_key, data_str) æˆ– (None, None)
    """
    from datetime import timedelta, timezone
    
    # ä»ä»Šå¤©å¼€å§‹å‘å‰æŸ¥æ‰¾ï¼ˆä½¿ç”¨åŒ—äº¬æ—¶é—´ï¼‰
    beijing_tz = timezone(timedelta(hours=8))
    current_date = datetime.now(beijing_tz)
    
    for i in range(max_days_back):
        check_date = current_date - timedelta(days=i)
        date_str = check_date.strftime('%Y-%m-%d')
        kv_key = f"qimai_data_{date_str}"
        
        print(f"ğŸ” æ£€æŸ¥æ—¥æœŸ: {date_str} (key: {kv_key})")
        data_str = kv_read(account_id, namespace_id, api_token, kv_key)
        
        if data_str:
            print(f"âœ… æ‰¾åˆ°æ•°æ®: {date_str}")
            return kv_key, data_str
        else:
            print(f"âŒ æœªæ‰¾åˆ°æ•°æ®: {date_str}")
    
    print(f"âš ï¸ å‘å‰æŸ¥æ‰¾äº† {max_days_back} å¤©ï¼Œæœªæ‰¾åˆ°ä»»ä½•æ•°æ®")
    return None, None

def setup_log_directory():
    """åˆ›å»ºæ—¥å¿—ç›®å½•ç»“æ„ï¼Œè¿”å›æ—¥å¿—ç›®å½•è·¯å¾„"""
    from datetime import timezone
    beijing_tz = timezone(timedelta(hours=8))
    now = datetime.now(beijing_tz)
    log_dir = os.path.join(
        'logs',
        str(now.year),
        f"{now.month:02d}",
        f"{now.day:02d}",
        f"{now.year}{now.month:02d}{now.day:02d}_{now.hour:02d}{now.minute:02d}{now.second:02d}"
    )
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(log_dir, exist_ok=True)
    print(f"ğŸ“ åˆ›å»ºæ—¥å¿—ç›®å½•: {log_dir}")
    
    return log_dir

def get_current_progress():
    """è·å–å½“å‰å¤„ç†è¿›åº¦"""
    checkpoint_file = os.path.join('logs', 'process_keywords_checkpoint.json')
    
    try:
        if os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
                print(f"ğŸ“Š å·²åŠ è½½å¤„ç†è¿›åº¦: {len(checkpoint_data.get('processed_keywords', []))} ä¸ªå…³é”®è¯")
                return checkpoint_data
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å¤„ç†è¿›åº¦æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
            return {"processed_keywords": [], "timestamp": datetime.now(beijing_tz).isoformat()}
    except Exception as e:
        print(f"âš ï¸ è¯»å–å¤„ç†è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
        return {"processed_keywords": [], "timestamp": datetime.now(beijing_tz).isoformat()}

def update_progress(processed_keywords, log_dir):
    """æ›´æ–°å¤„ç†è¿›åº¦"""
    checkpoint_file = os.path.join('logs', 'process_keywords_checkpoint.json')
    checkpoint_data = {
        "processed_keywords": processed_keywords,
        "timestamp": datetime.now(beijing_tz).isoformat()
    }
    
    # æ›´æ–°ä¸»è¿›åº¦æ–‡ä»¶
    with open(checkpoint_file, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    # åŒæ—¶ä¿å­˜åˆ°å½“å‰æ—¥å¿—ç›®å½•
    with open(os.path.join(log_dir, 'keywords_progress.json'), 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ å·²æ›´æ–°å¤„ç†è¿›åº¦: {len(processed_keywords)} ä¸ªå…³é”®è¯")

def process_keywords(max_process_count=None, max_rank_count=None, force_restart=False):
    """å¤„ç†å…³é”®è¯æ•°æ®ï¼Œä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆæ–‡ç« æ ‡é¢˜
    
    Args:
        max_process_count: æœ€å¤§å¤„ç†æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å…³é”®è¯
        max_rank_count: æœ€å¤§æ’è¡Œæ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ’è¡Œ
        force_restart: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¼€å§‹å¤„ç†
    """
    # æ·»åŠ è¿ç»­å¤±è´¥è®¡æ•°å™¨å’Œç†”æ–­æœºåˆ¶
    consecutive_failures = 0
    max_consecutive_failures = 5  # è¿ç»­å¤±è´¥5æ¬¡åç†”æ–­ï¼ˆä¸ArticleGeneratorä¿æŒä¸€è‡´ï¼‰
    
    # æ·»åŠ KVä¿å­˜å¤±è´¥è®¡æ•°å™¨
    kv_save_failures = 0
    max_kv_save_failures = 5  # KVä¿å­˜è¿ç»­å¤±è´¥5æ¬¡ååœæ­¢
    print("=== å¼€å§‹å¤„ç†å…³é”®è¯æ•°æ® ===")
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = setup_log_directory()
    
    # è®°å½•å¤„ç†è¯¦æƒ…
    keywords_details = {
        "start_time": datetime.now(beijing_tz).isoformat(),
        "max_process_count": max_process_count,
        "max_rank_count": max_rank_count,
        "force_restart": force_restart,
        "keywords": []
    }
    
    if max_process_count:
        print(f"ğŸ”¢ å…³é”®è¯é™åˆ¶æ¨¡å¼ï¼šæœ€å¤šå¤„ç† {max_process_count} ä¸ªå…³é”®è¯")
    else:
        print("ğŸš€ å…³é”®è¯æ­£å¼æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰å…³é”®è¯")
    
    if max_rank_count:
        print(f"ğŸ“Š æ’è¡Œé™åˆ¶æ¨¡å¼ï¼šæœ€å¤šå¤„ç†å‰ {max_rank_count} å")
    else:
        print("ğŸ“Š æ’è¡Œæ­£å¼æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ’è¡Œ")
    
    if force_restart:
        print("ğŸ”„ å¼ºåˆ¶é‡æ–°å¼€å§‹å¤„ç†æ¨¡å¼")
    
    # åˆå§‹åŒ–é…ç½®å’ŒAPIç®¡ç†å™¨
    config_manager = ConfigManager()
    api_manager = MultiPlatformApiManager(config_manager)
    
    # è®¾ç½®é»˜è®¤å¹³å°ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å¹³å°ï¼‰
    default_platform = config_manager.config.get("default_platform", "groq")
    api_manager.set_platform(default_platform)
    print(f"ä½¿ç”¨AIå¹³å°: {default_platform}")
    
    # ä»é…ç½®æ–‡ä»¶è·å–Cloudflare KV å‡­è¯
    kv_config = config_manager.config.get('kv_storage', {})
    ACCOUNT_ID = kv_config.get('account_id')
    NAMESPACE_ID = kv_config.get('namespace_id')
    API_TOKEN = kv_config.get('api_token')
    
    if not all([ACCOUNT_ID, NAMESPACE_ID, API_TOKEN]):
        print("âŒ KVå­˜å‚¨é…ç½®ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥config.jsonä¸­çš„kv_storageé…ç½®")
        return False
    
    # è·å–å…³é”®è¯å¤„ç†é…ç½®
    keyword_config = config_manager.config.get('keyword_processing', {})
    default_max_rank = keyword_config.get('default_max_rank_count', 1000)
    kv_search_days = keyword_config.get('kv_search_days_back', 30)
    
    # å¦‚æœæ²¡æœ‰è®¾ç½®max_rank_countï¼Œä½¿ç”¨é»˜è®¤å€¼
    if max_rank_count is None:
        max_rank_count = default_max_rank
        print(f"ğŸ“Š ä½¿ç”¨é»˜è®¤æ’è¡Œæ•°é‡é™åˆ¶: {max_rank_count}")
    
    # æŸ¥æ‰¾æœ€æ–°å­˜åœ¨çš„æ•°æ®ï¼ˆä»è¿œç¨‹è·å–æ‰€æœ‰å…³é”®è¯ï¼‰
    print("æ­£åœ¨ä»è¿œç¨‹æŸ¥æ‰¾KVå­˜å‚¨ä¸­æœ€æ–°å­˜åœ¨çš„æ•°æ®...")
    kv_key, existing_data_str = find_latest_kv_data(ACCOUNT_ID, NAMESPACE_ID, API_TOKEN, kv_search_days)
    
    if existing_data_str:
        print(f"âœ… æ‰¾åˆ°æœ€æ–°æ•°æ®ï¼Œç»§ç»­å¤„ç†... (key: {kv_key})")
        processed_data = json.loads(existing_data_str)
    else:
        print("ğŸ“ æœªæ‰¾åˆ°ä»»ä½•KVæ•°æ®ï¼Œä»JSONæ–‡ä»¶åŠ è½½åˆå§‹æ•°æ®...")
        with open('qimai_1_100_pages_20250924_161013.json', 'r', encoding='utf-8') as f:
            processed_data = json.load(f)
    
    # è·å–å½“å‰å·²å¤„ç†çš„å…³é”®è¯åˆ—è¡¨
    if force_restart:
        print("ğŸ”„ å¼ºåˆ¶é‡æ–°å¼€å§‹ï¼Œå¿½ç•¥å·²å¤„ç†è¿›åº¦")
        current_progress = {"processed_keywords": []}
    else:
        current_progress = get_current_progress()
    
    processed_keywords = current_progress.get("processed_keywords", [])
    
    # è·å–zh-twçš„æ ‡é¢˜ç”Ÿæˆæç¤ºè¯
    title_prompt = config_manager.config['google_seo_article_title_prompt']['zh-tw']['template']
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_keywords = 0
    success_count = 0  # æˆåŠŸå¤„ç†çš„å…³é”®è¯æ•°é‡
    skipped_count = 0
    failed_count = 0
    processed_count = 0  # å·²å¤„ç†çš„å…³é”®è¯æ€»æ•°ï¼ˆåŒ…æ‹¬è·³è¿‡ã€æˆåŠŸã€å¤±è´¥ï¼‰
    processed_ranks = 0  # å·²å¤„ç†çš„æ’è¡Œæ•°é‡
    
    # è®¡ç®—æ€»å…³é”®è¯æ•°ï¼Œç”¨äºæ˜¾ç¤ºè¿›åº¦
    all_keywords = []
    for page_key, page in processed_data['pages'].items():
        for item in page['wordRankList']:
            if 'word' in item:
                all_keywords.append(item['word'])
    
    print(f"âœ… è¿œç¨‹å…±è·å–åˆ° {len(all_keywords)} ä¸ªå…³é”®è¯")
    
    # å¦‚æœå·²è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡ï¼Œåˆ™ç›´æ¥è¿”å›
    if not force_restart and max_process_count and len(processed_keywords) >= max_process_count:
        print(f"ğŸ›‘ å·²è¾¾åˆ°æœ€å¤§å…³é”®è¯å¤„ç†æ•°é‡é™åˆ¶ ({max_process_count})ï¼Œæ— éœ€å¤„ç†")
        print(f"   å½“å‰å·²å¤„ç†å…³é”®è¯æ•°: {len(processed_keywords)}")
        
        # è®°å½•å¤„ç†ç»“æœ
        keywords_details["end_time"] = datetime.now(beijing_tz).isoformat()
        keywords_details["status"] = "skipped"
        keywords_details["message"] = f"å·²è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡ ({max_process_count})ï¼Œæ— éœ€å¤„ç†"
        keywords_details["stats"] = {
            "total_keywords": 0,
            "processed_ranks": 0,
            "processed_count": len(processed_keywords),
            "success_count": 0,
            "failed_count": 0,
            "skipped_count": len(processed_keywords)
        }
        
        # ä¿å­˜å¤„ç†è¯¦æƒ…åˆ°æ—¥å¿—ç›®å½•
        with open(os.path.join(log_dir, 'keywords_details.json'), 'w', encoding='utf-8') as f:
            json.dump(keywords_details, f, ensure_ascii=False, indent=2)
        
        return True
    
    # éå†æ‰€æœ‰é¡µé¢å’Œå…³é”®è¯
    print("\nå¼€å§‹å¤„ç†å…³é”®è¯...")
    for page_key, page in processed_data['pages'].items():
        print(f"\nå¤„ç†é¡µé¢: {page_key}")
        
        for item in page['wordRankList']:
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡ï¼ˆå¦‚æœè®¾ç½®äº†é™åˆ¶ï¼‰
            if max_process_count and processed_count >= max_process_count:
                print(f"ğŸ›‘ å·²è¾¾åˆ°æœ€å¤§å…³é”®è¯å¤„ç†æ•°é‡é™åˆ¶ ({max_process_count})ï¼Œåœæ­¢å¤„ç†")
                break
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¾¾åˆ°æœ€å¤§æ’è¡Œæ•°é‡ï¼ˆå¦‚æœè®¾ç½®äº†é™åˆ¶ï¼‰
            if max_rank_count and processed_ranks >= max_rank_count:
                print(f"ğŸ›‘ å·²è¾¾åˆ°æœ€å¤§æ’è¡Œæ•°é‡é™åˆ¶ ({max_rank_count})ï¼Œåœæ­¢å¤„ç†")
                break
            
            total_keywords += 1
            processed_ranks += 1
            
            # è·å–å…³é”®è¯
            keyword = item['word']
            rank = item.get('rank', 'N/A')
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°å¼€å§‹ï¼‰
            if not force_restart and keyword in processed_keywords:
                skipped_count += 1
                processed_count += 1  # è·³è¿‡ä¹Ÿè¦è®¡å…¥æ€»å¤„ç†æ•°é‡
                print(f"  [è·³è¿‡] å…³é”®è¯ '{keyword}' å·²å¤„ç†è¿‡")
                
                # æ›´æ–°å…³é”®è¯è¯¦æƒ…
                keyword_detail["status"] = "skipped"
                keyword_detail["reason"] = "å·²å¤„ç†è¿‡"
                keyword_detail["end_time"] = datetime.now(beijing_tz).isoformat()
                keywords_details["keywords"].append(keyword_detail)
                continue
            
            # åˆ›å»ºå…³é”®è¯è¯¦æƒ…è®°å½•
            keyword_detail = {
                "keyword": keyword,
                "rank": rank,
                "start_time": datetime.now(beijing_tz).isoformat()
            }
            
            # æ˜¾ç¤ºå¤„ç†è¿›åº¦ï¼ˆä½¿ç”¨processed_count + 1è¡¨ç¤ºå³å°†å¤„ç†çš„åºå·ï¼‰
            if max_process_count and max_rank_count:
                print(f"  [{processed_count + 1}/{max_process_count}] æ’è¡Œ[{processed_ranks}/{max_rank_count}] å¤„ç†å…³é”®è¯: {keyword} (æ’è¡Œ: {rank})")
            elif max_process_count:
                print(f"  [{processed_count + 1}/{max_process_count}] å¤„ç†å…³é”®è¯: {keyword} (æ’è¡Œ: {rank})")
            elif max_rank_count:
                print(f"  [{processed_count + 1}] æ’è¡Œ[{processed_ranks}/{max_rank_count}] å¤„ç†å…³é”®è¯: {keyword} (æ’è¡Œ: {rank})")
            else:
                print(f"  [{processed_count + 1}] å¤„ç†å…³é”®è¯: {keyword} (æ’è¡Œ: {rank})")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ ‡é¢˜
            need_generation = force_restart or 'titles' not in item or not item['titles']
            
            if need_generation:
                try:
                    # ç”Ÿæˆæ–‡ç« æ ‡é¢˜
                    generated_titles = generate_title(title_prompt, keyword, config_manager, api_manager)
                    
                    # æ£€æŸ¥ç”Ÿæˆçš„æ ‡é¢˜æ˜¯å¦æœ‰æ•ˆ
                    if isinstance(generated_titles, str) and (generated_titles.startswith("ç”Ÿæˆå¤±è´¥") or generated_titles.startswith("æ ¼å¼éªŒè¯å¤±è´¥")):
                        print(f"  âŒ å…³é”®è¯ '{keyword}' å¤„ç†å¤±è´¥: {generated_titles}")
                        consecutive_failures += 1
                        print(f"  ğŸ“Š è¿ç»­å¤±è´¥è®¡æ•°å™¨æ›´æ–°: {consecutive_failures}/{max_consecutive_failures}")
                        
                        # æ£€æŸ¥ç†”æ–­æ¡ä»¶
                        if consecutive_failures >= max_consecutive_failures:
                            print(f"ğŸ”¥ è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œè§¦å‘ç†”æ–­æœºåˆ¶ï¼Œåœæ­¢å¤„ç†")
                            
                            # ä¿å­˜å½“å‰çŠ¶æ€å¹¶é€€å‡º
                            keywords_details["circuit_breaker_triggered"] = True
                            keywords_details["consecutive_failures"] = consecutive_failures
                            keywords_details["end_time"] = datetime.now(beijing_tz).isoformat()
                            keywords_details["status"] = "circuit_breaker"
                            keywords_details["message"] = f"è¿ç»­å¤±è´¥{consecutive_failures}æ¬¡ï¼Œè§¦å‘ç†”æ–­æœºåˆ¶"
                            
                            # ä¿å­˜å¤„ç†è¯¦æƒ…
                            with open(os.path.join(log_dir, 'keywords_details.json'), 'w', encoding='utf-8') as f:
                                json.dump(keywords_details, f, ensure_ascii=False, indent=2)
                            
                            # è¿”å›å­—ç¬¦ä¸²è¡¨ç¤ºç†”æ–­çŠ¶æ€
                            return "circuit_breaker"
                        
                        # å³ä½¿å¤±è´¥ä¹Ÿè¦ä¿å­˜çŠ¶æ€
                        item['titles'] = []  # ç©ºæ ‡é¢˜åˆ—è¡¨
                        item['article_status'] = 'failed'
                        item['error_message'] = generated_titles
                        item['created_at'] = datetime.now(beijing_tz).isoformat()
                        
                        failed_count += 1
                        processed_count += 1  # å¤±è´¥å¤„ç†ä¹Ÿè¦è®¡å…¥æ€»å¤„ç†æ•°é‡
                        
                        # æ›´æ–°å…³é”®è¯è¯¦æƒ…
                        keyword_detail["status"] = "failed"
                        keyword_detail["error"] = generated_titles
                    else:
                        print(f"  âœ… å…³é”®è¯ '{keyword}' å¤„ç†æˆåŠŸï¼Œç”Ÿæˆäº† {len(generated_titles)} ä¸ªæ ‡é¢˜")
                        # åªåœ¨ç¡®å®æˆåŠŸæ—¶æ‰é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°å™¨
                        if consecutive_failures > 0:
                            print(f"  ğŸ”„ é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°å™¨: {consecutive_failures} -> 0")
                            consecutive_failures = 0
                        else:
                            print(f"  ğŸ“Š è¿ç»­å¤±è´¥è®¡æ•°å™¨ä¿æŒ: {consecutive_failures}")
                        
                        # åˆ›å»ºæ ‡é¢˜å¯¹è±¡åˆ—è¡¨
                        title_objects = []
                        for title_line in generated_titles:
                            # è§£ææ ‡é¢˜è¡Œï¼šæ–‡ç« æ ‡é¢˜----æ–‡ç« è‡ªå®šä¹‰å°¾è¯----æ¸¸æˆåç§°
                            parts = title_line.split('----')
                            if len(parts) == 3:
                                article_title, custom_suffix, game_name = parts
                                
                                # åˆ›å»ºæ ‡é¢˜å¯¹è±¡
                                title_obj = {
                                    "title": article_title.strip(),
                                    "custom_suffix": custom_suffix.strip(),
                                    "game_name": game_name.strip(),
                                    "created_at": datetime.now(beijing_tz).isoformat(),
                                    "last_used_at": None,  # ä¸Šæ¬¡ä½¿ç”¨æ—¶é—´
                                    "use_count": 0,  # ä½¿ç”¨æ¬¡æ•°
                                    "usage_records": []  # ä½¿ç”¨è®°å½•å¯¹è±¡åˆ—è¡¨
                                }
                                title_objects.append(title_obj)
                        
                        # æ·»åŠ æ–°å­—æ®µåˆ°å…³é”®è¯å¯¹è±¡
                        item['titles'] = title_objects  # ä¿å­˜æ ‡é¢˜å¯¹è±¡åˆ—è¡¨
                        item['article_status'] = 'generated'  # å…³é”®è¯æ•´ä½“çŠ¶æ€
                        item['created_at'] = datetime.now(beijing_tz).isoformat()  # åˆ›å»ºæ—¶é—´
                        success_count += 1  # åªæœ‰æˆåŠŸå¤„ç†æ‰è®¡æ•°
                        processed_count += 1  # æˆåŠŸå¤„ç†ä¹Ÿè¦è®¡å…¥æ€»å¤„ç†æ•°é‡
                        
                        # æ›´æ–°å…³é”®è¯è¯¦æƒ…
                        keyword_detail["status"] = "success"
                        keyword_detail["titles_count"] = len(title_objects)
                        
                        # æ›´æ–°å·²å¤„ç†å…³é”®è¯åˆ—è¡¨
                        if keyword not in processed_keywords:
                            processed_keywords.append(keyword)
                            # æ¯å¤„ç†æˆåŠŸä¸€ä¸ªå…³é”®è¯å°±æ›´æ–°è¿›åº¦
                            update_progress(processed_keywords, log_dir)
                    
                    # æ¯ä¸ªå…³é”®è¯å¤„ç†å®Œæˆåç«‹å³ä¿å­˜åˆ°KVå­˜å‚¨
                    print(f"  ğŸ’¾ ç«‹å³ä¿å­˜åˆ°KVå­˜å‚¨...")
                    try:
                        kv_write(ACCOUNT_ID, NAMESPACE_ID, API_TOKEN, kv_key, 
                                json.dumps(processed_data, ensure_ascii=False, indent=2))
                        print(f"  âœ… ä¿å­˜å®Œæˆ")
                        kv_save_failures = 0  # æˆåŠŸä¿å­˜åé‡ç½®å¤±è´¥è®¡æ•°
                    except Exception as kv_error:
                        kv_save_failures += 1
                        print(f"  âŒ KVå­˜å‚¨ä¿å­˜å¤±è´¥ (ç¬¬{kv_save_failures}æ¬¡): {str(kv_error)}")
                        
                        if kv_save_failures >= max_kv_save_failures:
                            print(f"ğŸ’¥ KVå­˜å‚¨è¿ç»­å¤±è´¥ {kv_save_failures} æ¬¡ï¼Œåœæ­¢å¤„ç†ä»¥é¿å…æ•°æ®ä¸¢å¤±")
                            
                            # ä¿å­˜é”™è¯¯çŠ¶æ€å¹¶é€€å‡º
                            keywords_details["kv_save_failures"] = kv_save_failures
                            keywords_details["end_time"] = datetime.now(beijing_tz).isoformat()
                            keywords_details["status"] = "kv_save_failure"
                            keywords_details["message"] = f"KVå­˜å‚¨è¿ç»­å¤±è´¥{kv_save_failures}æ¬¡"
                            
                            # ä¿å­˜å¤„ç†è¯¦æƒ…
                            with open(os.path.join(log_dir, 'keywords_details.json'), 'w', encoding='utf-8') as f:
                                json.dump(keywords_details, f, ensure_ascii=False, indent=2)
                            
                            return False
                    
                except ApiExhaustedRetriesError as e:
                    print(f"  ğŸ”¥ å…³é”®è¯ '{keyword}' APIé‡è¯•è€—å°½ï¼Œç«‹å³è§¦å‘ç†”æ–­æœºåˆ¶")
                    
                    # ä¿å­˜å½“å‰çŠ¶æ€å¹¶é€€å‡º
                    keywords_details["circuit_breaker_triggered"] = True
                    keywords_details["consecutive_failures"] = consecutive_failures + 1
                    keywords_details["end_time"] = datetime.now(beijing_tz).isoformat()
                    keywords_details["status"] = "circuit_breaker"
                    keywords_details["message"] = f"APIé‡è¯•è€—å°½å¼‚å¸¸ï¼Œç«‹å³è§¦å‘ç†”æ–­æœºåˆ¶: {str(e)}"
                    
                    # ä¿å­˜å¤„ç†è¯¦æƒ…
                    with open(os.path.join(log_dir, 'keywords_details.json'), 'w', encoding='utf-8') as f:
                        json.dump(keywords_details, f, ensure_ascii=False, indent=2)
                    
                    return "circuit_breaker"
                except Exception as e:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–ç†”æ–­ç›¸å…³å¼‚å¸¸
                    if any(keyword in str(e) for keyword in ['ç†”æ–­æœºåˆ¶', 'è¿ç»­å¤±è´¥', 'ğŸ”¥']):
                        print(f"  ğŸ”¥ å…³é”®è¯ '{keyword}' è§¦å‘ç†”æ–­æœºåˆ¶")
                        
                        # ä¿å­˜å½“å‰çŠ¶æ€å¹¶é€€å‡º
                        keywords_details["circuit_breaker_triggered"] = True
                        keywords_details["consecutive_failures"] = consecutive_failures + 1
                        keywords_details["end_time"] = datetime.now(beijing_tz).isoformat()
                        keywords_details["status"] = "circuit_breaker"
                        keywords_details["message"] = f"APIé‡è¯•è€—å°½å¼‚å¸¸ï¼Œç«‹å³è§¦å‘ç†”æ–­æœºåˆ¶: {str(e)}"
                        
                        # ä¿å­˜å¤„ç†è¯¦æƒ…
                        with open(os.path.join(log_dir, 'keywords_details.json'), 'w', encoding='utf-8') as f:
                            json.dump(keywords_details, f, ensure_ascii=False, indent=2)
                        
                        return "circuit_breaker"
                    
                    print(f"  ğŸ’¥ å…³é”®è¯ '{keyword}' æ¥å£æŠ¥é”™: {str(e)}")
                    consecutive_failures += 1
                    print(f"  ğŸ“Š è¿ç»­å¤±è´¥è®¡æ•°å™¨æ›´æ–°: {consecutive_failures}/{max_consecutive_failures}")
                    
                    # æ£€æŸ¥ç†”æ–­æ¡ä»¶
                    if consecutive_failures >= max_consecutive_failures:
                        print(f"ğŸ”¥ è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œè§¦å‘ç†”æ–­æœºåˆ¶ï¼Œåœæ­¢å¤„ç†")
                        
                        # ä¿å­˜å½“å‰çŠ¶æ€å¹¶é€€å‡º
                        keywords_details["circuit_breaker_triggered"] = True
                        keywords_details["consecutive_failures"] = consecutive_failures
                        keywords_details["end_time"] = datetime.now(beijing_tz).isoformat()
                        keywords_details["status"] = "circuit_breaker"
                        keywords_details["message"] = f"æ¥å£å¼‚å¸¸è¿ç»­å¤±è´¥{consecutive_failures}æ¬¡ï¼Œè§¦å‘ç†”æ–­æœºåˆ¶"
                        
                        # ä¿å­˜å¤„ç†è¯¦æƒ…
                        with open(os.path.join(log_dir, 'keywords_details.json'), 'w', encoding='utf-8') as f:
                            json.dump(keywords_details, f, ensure_ascii=False, indent=2)
                        
                        return "circuit_breaker"
                    
                    failed_count += 1
                    processed_count += 1  # å¼‚å¸¸å¤„ç†ä¹Ÿè¦è®¡å…¥æ€»å¤„ç†æ•°é‡
                    # ä¿å­˜é”™è¯¯çŠ¶æ€
                    item['titles'] = []
                    item['article_status'] = 'failed'
                    item['error_message'] = f"æ¥å£æŠ¥é”™: {str(e)}"
                    item['created_at'] = datetime.now(beijing_tz).isoformat()
                    
                    # æ›´æ–°å…³é”®è¯è¯¦æƒ…
                    keyword_detail["status"] = "error"
                    keyword_detail["error"] = str(e)
                    
                    # ä¿å­˜åˆ°KVå­˜å‚¨
                    try:
                        kv_write(ACCOUNT_ID, NAMESPACE_ID, API_TOKEN, kv_key, 
                                json.dumps(processed_data, ensure_ascii=False, indent=2))
                        kv_save_failures = 0  # æˆåŠŸä¿å­˜åé‡ç½®å¤±è´¥è®¡æ•°
                    except Exception as save_error:
                        kv_save_failures += 1
                        print(f"  âŒ KVå­˜å‚¨ä¿å­˜å¤±è´¥ (ç¬¬{kv_save_failures}æ¬¡): {str(save_error)}")
                        
                        # æ›´æ–°å…³é”®è¯è¯¦æƒ…
                        keyword_detail["kv_save_error"] = str(save_error)
                        
                        if kv_save_failures >= max_kv_save_failures:
                            print(f"ğŸ’¥ KVå­˜å‚¨è¿ç»­å¤±è´¥ {kv_save_failures} æ¬¡ï¼Œåœæ­¢å¤„ç†")
                            
                            # ä¿å­˜é”™è¯¯çŠ¶æ€å¹¶é€€å‡º
                            keywords_details["kv_save_failures"] = kv_save_failures
                            keywords_details["end_time"] = datetime.now(beijing_tz).isoformat()
                            keywords_details["status"] = "kv_save_failure"
                            keywords_details["message"] = f"KVå­˜å‚¨è¿ç»­å¤±è´¥{kv_save_failures}æ¬¡"
                            
                            # ä¿å­˜å¤„ç†è¯¦æƒ…
                            with open(os.path.join(log_dir, 'keywords_details.json'), 'w', encoding='utf-8') as f:
                                json.dump(keywords_details, f, ensure_ascii=False, indent=2)
                            
                            return False
            else:
                skipped_count += 1
                processed_count += 1  # è·³è¿‡å·²æœ‰æ ‡é¢˜ä¹Ÿè¦è®¡å…¥æ€»å¤„ç†æ•°é‡
                print(f"  [è·³è¿‡] å…³é”®è¯ '{keyword}' å·²æœ‰æ ‡é¢˜")
                
                # æ›´æ–°å…³é”®è¯è¯¦æƒ…
                keyword_detail["status"] = "skipped"
                keyword_detail["reason"] = "å·²æœ‰æ ‡é¢˜"
            
            # å®Œæˆå¤„ç†æ—¶é—´
            keyword_detail["end_time"] = datetime.now(beijing_tz).isoformat()
            keywords_details["keywords"].append(keyword_detail)
        
        # å¦‚æœå·²è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡ï¼Œè·³å‡ºå¤–å±‚å¾ªç¯ï¼ˆå¦‚æœè®¾ç½®äº†é™åˆ¶ï¼‰
        if max_process_count and processed_count >= max_process_count:
            break
        
        # å¦‚æœå·²è¾¾åˆ°æœ€å¤§æ’è¡Œæ•°é‡ï¼Œè·³å‡ºå¤–å±‚å¾ªç¯ï¼ˆå¦‚æœè®¾ç½®äº†é™åˆ¶ï¼‰
        if max_rank_count and processed_ranks >= max_rank_count:
            break
    
    # è¾“å‡ºå¤„ç†ç»“æœç»Ÿè®¡
    print("\n=== å¤„ç†å®Œæˆ ===")
    print(f"æ€»å…³é”®è¯æ•°: {total_keywords}")
    print(f"å·²å¤„ç†æ’è¡Œæ•°: {processed_ranks}")
    print(f"æ€»å¤„ç†å…³é”®è¯æ•°: {processed_count} (åŒ…æ‹¬æˆåŠŸã€å¤±è´¥ã€è·³è¿‡)")
    print(f"æˆåŠŸå¤„ç†å…³é”®è¯: {success_count}")
    print(f"å¤„ç†å¤±è´¥å…³é”®è¯: {failed_count}")
    print(f"è·³è¿‡å…³é”®è¯: {skipped_count}")
    if kv_key:
        print(f"æ•°æ®å·²ä¿å­˜åˆ°KVå­˜å‚¨ (key: {kv_key})")
    else:
        print("æ•°æ®æœªä¿å­˜åˆ°KVå­˜å‚¨ï¼ˆä½¿ç”¨æœ¬åœ°JSONæ–‡ä»¶ï¼‰")
    
    # æ˜¾ç¤ºAPIä½¿ç”¨ç»Ÿè®¡
    api_manager.show_usage_stats()
    
    # æ›´æ–°å¤„ç†è¯¦æƒ…
    keywords_details["end_time"] = datetime.now(beijing_tz).isoformat()
    keywords_details["stats"] = {
        "total_keywords": total_keywords,
        "processed_ranks": processed_ranks,
        "processed_count": processed_count,
        "success_count": success_count,
        "failed_count": failed_count,
        "skipped_count": skipped_count
    }
    
    # ä¿å­˜å¤„ç†è¯¦æƒ…åˆ°æ—¥å¿—ç›®å½•
    with open(os.path.join(log_dir, 'keywords_details.json'), 'w', encoding='utf-8') as f:
        json.dump(keywords_details, f, ensure_ascii=False, indent=2)
    
    # è¿”å›å¤„ç†ç»“æœ
    if success_count > 0 or skipped_count > 0:
        print(f"âœ… å¤„ç†æˆåŠŸï¼šæˆåŠŸå¤„ç†äº† {success_count} ä¸ªå…³é”®è¯ï¼Œè·³è¿‡äº† {skipped_count} ä¸ªå…³é”®è¯")
        return True
    else:
        print("âŒ å¤„ç†å¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å…³é”®è¯")
        return False

def test_process_keywords():
    """æµ‹è¯•æ–¹æ³•ï¼šå¤„ç†å°‘é‡å…³é”®è¯ç”¨äºæµ‹è¯•"""
    print("ğŸ§ª å¯åŠ¨æµ‹è¯•æ¨¡å¼...")
    process_keywords(max_process_count=3, max_rank_count=5)  # åªå¤„ç†3ä¸ªå…³é”®è¯ï¼Œå‰5åè¿›è¡Œæµ‹è¯•

def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='å¤„ç†å…³é”®è¯æ•°æ®ï¼Œä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆæ–‡ç« æ ‡é¢˜')
    parser.add_argument('--max-count', type=int, help='æœ€å¤§å¤„ç†å…³é”®è¯æ•°é‡')
    parser.add_argument('--max-rank', type=int, help='æœ€å¤§å¤„ç†æ’è¡Œæ•°é‡ï¼ˆå‰Nåï¼‰')
    parser.add_argument('--force-restart', action='store_true', help='å¼ºåˆ¶é‡æ–°å¼€å§‹å¤„ç†')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    if args.test:
        test_process_keywords()
    else:
        process_keywords(max_process_count=args.max_count, max_rank_count=args.max_rank, force_restart=args.force_restart)

if __name__ == "__main__":
    main()